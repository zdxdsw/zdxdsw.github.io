

<strong>Hi!</strong>
<ul>
    <li><strong>Mar 29 update: </strong> 
        <ul>
            The current running question-writing batch is the last one. 
            Validation HITs are running slowly and there are 4,000 more waiting in line. 
            Thank you all for the hard work, valuable feedback and fantastic suggestions on image categories suitable for this task. 
            "Success is defined by both me and my Turkers" is such a wonderful lesson to learn. 
            It's reciprocal and I'm thrilled to see genuine progress.

            The good news is our lab is also planning to start collecting a pure text version of this dataset in April 
            (with a similar size). The task basics will be the same, which is to create question-answer pairs, 
            but the given prompts will be text snippets. It would be great if you could also help us with the new task!
            Wish you all the best.
                 
        </ul>
    </li>
    
    <br>
    <li>New image categories will show up in this batch: "ethnic clothing", "cars", "festival", "mushrooms", "street murals" and "musical instruments".
        Positive and negative comments on these new categories are equally welcome!
    </li>
    <li>All of your three annotations should be based on <strong>either</strong> the original set <strong>or</strong> the backup set but not both. 
        For example, annotation 1 using the original set and annotation 2 using the backup set is <strong>not</strong> allowed. 
        Note that once you toggle, the image thumbnails next to <strong>all three</strong> annotation blocks would change together. 
        Also, "cross set question" is <strong>not</strong> allowed. 
        Sorry if the instructions were confusing previously.
    </li>
    <li>We 're trying out new image categories such as animals and plants. We understand that some trial and error is required at the beginning but we are curious about what novel directions these new categories will lead to.
        Here are some hints and caveats.  Originally we thought there is a lot to compare about appearances or habitats, since these types of knowledge would be the least time-sensitive. 
        But some Turkers said most of the information about appearances or habitats are already available with text search.  
        Specifically, someone asked how picky they should be when deciding if an answer can be found by a text search. 
        First of all, we do agree that appearances about animals or flowers are retrievable facts from text. 
        But this kind of information is also readily available in visual data. 
        It is true that a pure text search engine could be able to figure out answers if it had access to the entire web of text. 
        But apparently, at least for humans, doing image search is a shorter and more straightforward path to arrive at the answer, 
        since we can switch between text modality and image modality smoothly. 
        So imagine that if there were a search engine that could also do this sort of cross-modality reasoning + info aggregation, 
        then it would be happier to find answers in images for appearance-based questions. 
        <br>
        The above argument might be more applicable for comparison-based multi-image questions, 
        such as asking about a visual property that A has but B not, or which one of A and B has a more prominent property, 
        i.e. longer, thinner, more resembles a certain everyday object, higher color contrast, warmer color tones, etc. 
    </li>
    <!--
    <li><span style="color:rgb(1, 158, 67);"><strong>Backup set: </strong></span>We totally understand that occationally the image prompts are too bad to yield meaningful annotations. 
        Since there is technical difficulty to let you "skip a assignment", we add a "backup set" for each assignment. 
        Please feel free to toggle between the original set and the backup set but you can't use both at the same time.
    </li>
    <li><span style="color:rgb(1, 158, 67);"><strong>Thumbnail images: </strong></span>Previouly we mentioned the image indexing mistake. In order to alleviate this problem, we make a thumbnail show up once you select an image index. 
        Note that if you switch to the "backup set", the thumbnails would change accordingly! 
        Please double check whether your image selections are correct before you submit. Thank you in advance!
    </li>
    -->
    <li>For those buildings/people/streets... names, copying them from Wikipedia pages is fine but we want to restrict the dataset to <span style="color:rgb(218, 1, 1);">latin characters</span>. Thank you so much! </li>
    <!--
    <li>We find that some Turkers are making mistakes with image indexes. This would propagate error to our post-processing stage and your super nice annotations could be lost! 
        So please double check that you are selecting the correct image indexes! Images 1,2,3 are in the first row and images 4,5,6 are in the second row. Thank you so much! 
    </li>
    -->
    <!--<li><span style="color:rgb(0, 98, 255);">We are actively finding new turkers because we're trying to scale up the dataset by an order of magnitude. 
        We'd appreciate it if you could try and get more turkers do our qualification task. Thank you in advance!</span><br></li>-->
    <li>Contact us at <a href="mturk@yonatanbisk.com">mturk@yonatanbisk.com</a>
    </li>
    <li>Please select <b>exactly two</b> images for the "multi-image" annotation. (optional for annotation 1 & 2, required for annotation 3) </li>
    <li>We are currently assuming 10-15 min / task, but we will try and adjust appropriately!</li>
</ul>
<br>
Please let us know if any aspect of this HIT is unclear. We will try and iterate together.<br>
    
